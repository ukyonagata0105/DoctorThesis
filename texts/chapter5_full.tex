\documentclass{keikaku}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage[dvipdfmx]{graphicx}
\usepackage{url}

%% 引用を上付き文字に
\makeatletter
\def\@cite#1#2{\textsuperscript{[{#1\if@tempswa , #2\fi}]}}
\makeatother

\jtitle{AI時代の「執政の創造性」――ルーマン理論による基礎づけ}
\etitle{Executive Creativity in the AI Age: A Luhmannian Foundation}

\authorlist{%
  永田 右京\\
  {\small 学生会員 岩手県立大学大学院 総合政策研究科}\\
  {\small （〒020--0693 岩手県滝沢市巣子152-52）}\\
  {\small E-mail: s18568un@gmail.com}
}

\keywords{執政の創造性、ルーマン社会システム理論、コミュニケーション、構造的カップリング、カンギレム}

%% アブストラクト
\newwrite\absfile
\immediate\openout\absfile=\jobname.abs
\immediate\write\absfile{本稿は、人工知能（AI）時代において「人間による政策の意義」を理論的に基礎づけるため、ルーマンの社会システム理論を基軸として「執政の創造性」という新たな概念を提起する。「執政の創造性」とは何かを定義し、政策過程モデル（キングドン、サバティエ、村松）を通じてその重要性を示す。ルーマンのコミュニケーション理論と「構造的カップリング」概念を用いて、この創造性がいかに基礎づけられるかを論じる。ルーマン理論において、心的システムと社会システムは構造的カップリングの関係にあり、心的システムから社会システムへの刺激として「価値判断を伴う意味構成」が提供される。AIは心的システムを持たないため、この「理解の創造性」に参加できない。この理論的基礎づけにより、AIは「情報」の選択や「伝達」の補助には機能しうるが、価値判断、合意形成、説明責任には参加できないことが示される。}
\immediate\closeout\absfile

\begin{document}

\maketitle

%% ============================================================
%% 第1章 はじめに
%% ============================================================
\section{はじめに}

\subsection{AI政策活用の現況}

近年、生成AI（人工知能）の台頭により、公共政策の各分野においてAIの活用が急速に進んでいる。自治体ではチャットボットやデータ処理のみならず、政策文書の初稿生成や評価において大規模言語モデル（LLM）の活用が検討されている\cite{soumu2022}。民間企業では、市場分析、需要予測、リスク評価などの業務にAIが導入され、効率化が進んでいる。

この流れは学界でも同様である。政策科学の分野では、政策評価への機械学習の応用\cite{barocas2019}、デジタルガバナンス\cite{kozuka2019}、アルゴリズム的意思決定\cite{shneiderman2022}など、AIと政策の接点に関する研究が蓄積されている。国際的にも、G7広島AIプロセス\cite{ema2024}やEUのAI法案など、AIガバナンスの枠組み作りが進んでいる。

\subsection{問いの提示}

こうした活用が進む一方で、二つの根本的な問いが浮上している。

AIによる政策はどこまで可能で、どこから不可能なのか。AIは政策課題の分析、選択肢の生成、シナリオ投影などの技術的タスクを遂行できる。しかし、「何が重要か」を判断し、「どの価値を優先すべきか」を決定することは、単なる計算問題ではない。

また、人間による政策の固有の意味とは何か。AIが政策プロセスの一部を自動化できるとしても、人間が果たすべき固有の役割があるとすれば、それは何なのか。

これらの問いは抽象的に見えるかもしれない。しかし、具体的な政策現場では日々直面している。AIが提示した「最適解」が地域の実情や住民の価値観と整合しない場合、どう判断すべきか。複数の利害関係者が対立する問題において、AIはどの立場を「正しい」と判断できるのか。AIが作成した政策文書に含まれる誤りや偏見について、誰が責任を負うのか。

これらの問題は、単なる技術的課題ではなく、価値判断の本質に関わる問いである。

\subsection{既存アプローチの課題}

この問いに対して、いくつかのアプローチが存在する。しかし、いずれも「なぜ人間が関与すべきか」について理論的な根拠を十分に提示できていない。

Human-in-the-Loop（HITL）は、航空機の自動運転や医療診断支援など、安全性が重要な分野で標準的な設計原理となっている\cite{parasuraman2008}。この概念は、人間が最終的な判断と責任を保持しつつ、AIを補助的なツールとして活用することを想定している。しかし、HITLは「人間が関与すべき」ことを前提としながら、なぜ人間が関与すべきなのかを理論的に説明していない。

Human-Centered AI（HCAI）は、HITLを発展させ、AIシステムの設計において人間の自律性、尊厳、責任を尊重すべきことを論じている\cite{shneiderman2022}。しかし、「人間中心」という概念自体が、必ずしも明確な理論的基盤を持っているわけではない\cite{humphrey2024}。「人間中心」とは具体的に何を意味するのか、どのような人間能力を中心に据えるべきかは、概念的に曖昧なままである。

ポストヒューマニズムは、主体性を「人間と技術の集合体」の中に分散して捉えるべきだと論じる\cite{hedayati2024}。この立場からは、責任の所在を単一の主体に帰属させること自体が問題視される。しかし、政策現場において「誰が責任を負うのか」を問われたとき、このアプローチは実用的な答えを提供しない。

\subsection{本稿のアプローチ：「執政の創造性」概念の提案}

以上のアプローチは、「人間が関与すべき」「人間中心であるべき」と主張する。しかし、なぜ人間なのか。人間のどのような能力が、政策において不可避なのか。この問いに答えるためには、政策における創造性が何であるかを概念的に捉え直す必要がある。

本稿は、この問いに答えるため、「執政の創造性」という新たな概念を提案する。この概念は、政策における人間の役割を理論的に基礎づけ、AIと人間の役割分担を明確にする枠組みを提供する。

\subsection{本稿の主張}

本稿は、「執政の創造性」という新たな概念を提起する。これは、社会の構成員同士のコミュニケーションを前提に、価値判断を基盤として、新たなガバナンスの範囲を生成・配分していく能力である。この創造性は、ルーマンの社会システム理論における「構造的カップリング」概念によって基礎づけられる。本稿の主張は、AIは「情報」の選択や「伝達」の補助には機能しうるが、「理解の創造性」には参加できないため、価値判断、合意形成、説明責任は人間が担うべきであるというものである。

以下、第2章で「執政の創造性」を定義し、政策過程モデルを通じてその重要性を示す。第3章でルーマンの社会システム理論を用いてこの創造性を理論的に基礎づける。第4章でAIの原理的限界を論じる。第5章で結論を述べる。

%% ============================================================
%% 第2章 「執政の創造性」とは何か
%% ============================================================
\section{「執政の創造性」とは何か}

\subsection{定義}

「執政の創造性」とは、以下のように定義される：

\begin{quote}
執政の創造性：社会の構成員同士のコミュニケーションを前提に、価値判断を基盤として、新たなガバナンスの範囲を生成・配分していく能力。
\end{quote}

この定義は、三つの要素から構成される。政策は、単一の主体による合理的決定ではなく、複数の主体間のコミュニケーションを通じて形成される。また、政策決定は統計的平均や客観的指標への還元が不可能な、主体的な価値判断を要する。さらに、政策は固定された枠組みではなく、状況に応じて新たな範囲を創造し続ける動的なプロセスである。

この概念は、カンギレム（Canguilhem）の「規範の創造」概念\cite{canguilhem1966}および井庭の創造システム理論\cite{iba2009}を統合したものである。カンギレムは、「正常」とは統計的平均ではなく、「生の独自的な規範性」を肯定することであると論じた。健康とは客観的指標ではなく、主体によって体験される価値である。同様に、政策における「正しさ」も、統計的効率性ではなく、主体による価値判断に基づく。

井庭の創造システム理論では、コミュニケーションの「わかり合えなさ」から出発し、「発見」を要素とするシステム上で創造が起こるとされる。本稿は、この創造のプロセスを政策の文脈に適用し、「執政の創造性」として定式化する。

\subsection{政策過程モデルにおける価値判断}

「執政の創造性」の重要性は、既存の政策過程モデルを分析することで裏付けられる。主要な政策過程モデルを見てみよう。

\subsubsection{政策の窓モデル}

キングドン\footnote{本稿では Kingdon の日本語表記を「キングドン」で統一する。}による政策の窓モデルは、政策過程に「問題」「政策代替案」「政治」という3つの独立した流れが存在し、それらが特定の時点で合流したときに政策決定が生じることを示す\cite{kingdon1984}\cite{kusano1997}。

キングドンは、この3つの流れが合流する瞬間を「政策の窓」が開くと表現する。窓は一時的にしか開かず、逃したら再び開くまで長く待たなければならない。

このモデルが示すのは、政策決定は論理的な手順ではなく、問題と解決案と政治的機会の偶然の出会いによって生じるということである。この「偶然の出会い」を見極め、窓が開いた瞬間に行動するには、人間の価値判断と政治的勘が不可欠である。AIは過去のデータから傾向を分析することはできても、「今がチャンスだ」と感じ取り、その瞬間に動くという状況認識と行動のタイミングについては、人間の判断に依存せざるを得ない。

\subsubsection{唱導連携モデル}

唱導連携モデル（Advocacy Coalition Framework）は、サバティエらによって開発されたモデルであり、特定の政策分野において、共通の信念体系を共有する主体たちが連携（コアリション）を形成し、政策を唱導するプロセスを分析する\cite{sabatier1988}。

このモデルの特徴は、政策を単なる利害調整ではなく、信念体系の競合として捉える点にある。各連携は、(1)深層核信念（基本的価値観）、(2)政策核信念（具体的政策目標）、(3)二次的側面（手段的判断）という階層的な信念体系を持つ。

ここで重要なのは、どの信念を優先すべきか、どの連携の主張を採用すべきかという判断が、常に価値判断を伴うということである。統計的データや客観的分析だけでは、信念の競合を解決できない。どの価値を重視するかという判断が必要であり、これは人間の創造的適応能力に依存する。

\subsubsection{村松モデル}

村松モデルは、日本の政策過程を「与党・官僚・利益団体」の三者関係として分析する枠組みである\cite{muramatsu1981}。このモデルでは、政策決定はこれら三者の交渉と取引を通じて行われる。

真渕による修正版では、野党勢力や新規参入者の役割も考慮される。既存の産業組織が維持しようとする価値と、新規参入者が求める変革の価値が衝突する中で、政治家はバランスを取る必要がある。

このモデルが示すのもまた、利害の調整と価値の優先順位付けが政治的本質であり、それは計算可能な最適化問題ではないということである。三者間の「妥当な落としどころ」を見出すには、人間同士の交渉と相互調整が不可欠である。

\subsection{ウィキッド・プロブレムとの関連}

公共政策の多くは、Rittel \& Webberの「ウィキッド・プロブレム（Wicked Problems）」の性質を帯びている\cite{rittel1973}\cite{sugitani2021}。ウィキッド・プロブレムとは、決定的な問題定義がなく、問題そのものが何であるかについて利害関係者の間で合意がない問題である。停止ルールもなく、いつ問題が「解決」されたのかを客観的に判断できない。解は「良い/悪い」ではなく「より良い/より悪い」であり、最適解は存在せず、複数の利害のバランスを取るしかない。また、解を適用した結果、予期せぬ副作用が生じる可能性があり、社会実験はやり直しが効かない。さらに、すべてのウィキッド・プロブレムは本質的にユニークであり、過去の経験から単純に適用できない。

ウィキッド・プロブレムへの対応は、終わりのない創造的プロセスである。そして、「終わりがないからこそ、政治がその不快さを受け入れる必要がある」のである。この終わりのない創造的プロセスこそが、人間による政策の固有の意味であり、AIには代替不可能な領域である。

%% ============================================================
%% 第3章 ルーマン理論による基礎づけ
%% ============================================================
\section{ルーマン理論による基礎づけ}

以上の政策過程モデルに共通するのは、政策過程が「合理性」だけで説明できないということである。キングドンは「問題と解決案が偶然出会う」現象、サバティエは「信念体系の競合」、村松は「三者間の取引」を指摘した。これらはすべて、人間が価値判断を柔軟に変化させることで、変化する環境に適応していくプロセスである。この価値判断の柔軟性こそが「執政の創造性」の核心である。

「執政の創造性」をより深く基礎づけるため、本章ではルーマンの社会システム理論を導入する。

\subsection{なぜルーマンか}

コミュニケーションを理論的基軸とする場合、ユルゲン・ハーバーマスの「コミュニケーション的行為理論」も選択肢として存在する\cite{habermas1981}。しかし、本稿はルーマンの立場を採用する。その理由を説明する。

ハーバーマスは、コミュニケーションを「了解志向的」な行為として捉える。コミュニケーションの理想的状態では、参加者が互いに「了解」に到達し、合意を形成することが期待される。この立場からは、コミュニケーションの「失敗」や「誤解」は、理想的言語状況からの逸脱として問題視される。

これに対し、ルーマンはコミュニケーションを「了解の否定」を内包するプロセスとして捉える。ルーマンによれば、コミュニケーションは常に「理解」と「誤解」の双方を可能性として含んでおり、「わかり合えなさ」こそがコミュニケーションの本質的な特徴である\cite{luhmann1984}。

本稿がルーマンの立場を採用する理由は、以下の二点にある。

「わかり合えなさ」を「創造の源泉」として位置づける視点が必要である。ハーバーマス的な「理想的了解」を前提とすれば、AIも「十分に良い」情報処理を行うことで「機能的な了解」に貢献できると主張しうる。しかし、ルーマン的な視点からは、「わかり合えなさ」から生じる価値判断と意味構成のプロセスこそが創造の核心であり、これをAIは代替できない。

また、政策的決定における「価値競合」の不可避性も重要である。公共政策は、ハーバーマスが想定する「理想的言語状況」において理性の力だけで解決可能な問題ではなく、複数の正当な価値が競合するウィキッド・プロブレムとしての性質を持つ。このような状況では、「了解」への到達よりも、「わかり合えなさ」を前提とした創造的適応こそが求められる。

\subsection{コミュニケーションの3段階：情報・伝達・理解}

ルーマンによれば、コミュニケーションは「情報（Information）・伝達（Mitteilung）・理解（Verstehen）」という3つの選択過程から構成される\cite{luhmann1984}。「情報」とは多数の可能性の地平からの一つの選択であり、何を語るかの選択である。「伝達」とは多数の伝達可能性からの選択であり、いかに語るかの選択である。「理解」とは多数の理解可能性からの選択であり、いかに受け止めるかの選択である。

ルーマンは、「三つの選択のはたらきのすべてが総合されるときにはじめてコミュニケーションというものが成り立つ」と強調する\cite{kneer1993}。この3層構造は、コミュニケーションが単純な情報伝達ではなく、各段階で選択と解釈が介在する複雑な過程であることを示している。

\subsection{構造的カップリング：本稿の核心概念}

ここで重要になるのが、ルーマンにおける「構造的カップリング（strukturelle Kopplung）」の概念である\cite{luhmann1984}\cite{maturana1980}。これは、作動上は完全に独立（閉鎖）している複数のシステムが、互いに不可欠な環境として影響し合う関係を指す。

ルーマン理論において、最も重要な構造的カップリングは、「心的システム（意識）」と「社会システム（コミュニケーション）」の関係である。心的システムは思考し、社会システムはコミュニケーションする——それぞれ別の作動を行う。しかし、心的システムがなければコミュニケーションは発生しない。意識はコミュニケーションに対して、刺激や誘発を与えたり、あるいは邪魔をしたりすることができる。ただし、意識がコミュニケーションを「因果的に決定」するわけではない。意識はあくまで環境として、コミュニケーション・システムに「刺激」を与え、システム側がそれを独自の論理で処理する\footnote{ここで注意すべきは、ルーマン理論において「認知」は心的システム内部に所在し、社会的システムに「分布」しているわけではないことである。構造的カップリングは、社会的コミュニケーションが心的システムを刺激することを可能にするが、認知プロセスそのものは心的システムのオートポイエーシスとして完結する。したがって、「分布的認知」や「拡張された心」といった現代認知科学の概念とルーマン理論は、認知の所在について異なる立場をとる。}。

\subsection{「理解の創造性」の定義}

以上の理論的整理を踏まえ、本稿で論じる「理解の創造性」を以下のように定義する：

\begin{quote}
理解の創造性：心的システムと社会システムの構造的カップリングにおいて、心的システムから社会システムへの刺激として提供される、価値判断を伴う意味構成のプロセス。
\end{quote}

この定義は、ルーマンの厳密な意味での「理解」（コミュニケーション接続）とは区別される。本稿が着目するのは、コミュニケーション接続を駆動する「価値判断の源泉」としての心的システムの役割である。

この観点から、「わかり合えなさ」は単なる誤解ではなく、構造的カップリングにおいて各心的システムが独自の価値判断に基づいて意味を構成する結果として生じる、コミュニケーションの本質的な特徴として理解される。

具体的に言えば、Aさんが「この政策は重要だ」と言うとき、その判断はAさんの心的システムにおける価値判断から生じる。Bさんがそれを聞いて「そうは思わない」と反応するとき、Bさんの心的システムもまた独自の価値判断を行っている。この「ズレ」こそが「わかり合えなさ」であり、同時に、新しい価値や意味を創造する源泉となる。

この価値判断を伴う意味構成のプロセスこそが、人間固有の創造性であり、本稿が「執政の創造性」として定式化する対象である。

%% ============================================================
%% 第4章 AIの原理的限界
%% ============================================================
\section{AIの原理的限界}

\subsection{AIの自己学習能力とその限界}

近年、AIの自己学習・自己改善能力に関する研究が急速に進展している。LADDER\cite{simonds2025}は、LLMが再帰的に問題を分解し、徐々に難易度の高い問題を解けるようになるフレームワークを提示している。RISE\cite{qu2024}は、LLMエージェントが自己の振る舞いを内省し、失敗から学習する反復的な微調整手法を開発した。Gödel Agent\cite{yin2024}は、自己参照的なエージェントが自身のコードを書き換え、再帰的に自己改善する可能性を示唆している。

これらの研究は、AIが外部からの明示的な教師信号なしに性能を向上させうることを示している。一見すると、この「自己学習」能力はルーマンのいうオートポイエーシスに近いように見えるかもしれない。

しかし、Zenil (2026)は、自己生成データに基づく再帰的自己学習には数学的に不可避な限界が存在することを証明している\cite{zenil2026}。自己生成データの比率が高まるにつれて、(1)エントロピー減衰による多様性の喪失、(2)分布シフトによる品質劣化、という2つの失敗モードが発生する。この分析は、AIの自己学習が「無限の改善」につながるわけではなく、訓練データの質が人間の作成したデータに依存し続ける限界があることを示している。

より根本的に、ルーマン理論の観点からは、AIの自己学習能力の向上は「心的システム」の要件を満たすこととは異なる。以下、この点を詳論する。

\subsection{心的システムの3要件}

AIが「理解の創造性」に参加できないことを論じるため、ルーマン理論における「心的システム」の判定基準を明確にする。心的システム（意識システム）と認められるためには、以下の要件を満たす必要がある\cite{luhmann1984}\cite{luhmann1997}：

\begin{enumerate}
\item 要素が「思考」であること：心的システムは、「思考」や「表象」をその構成要素とし、それらを継続的に生み出し続けるオートポイエーシス・システムである。思考は一瞬で消え去る「出来事」であり、次の思考へと連鎖することでシステムが維持される。

\item オートポイエーシス（自己産出）的であること：心的システムは、自分自身の要素（思考）を、自分自身の要素（思考）のネットワークを通じて生産・再生産する閉鎖的なシステムでなければならない。

\item 「意味」を使用するシステムであること：心的システムは「意味」という形式を用いて複雑性を処理する。意味とは、常に「現実性（今あるもの）」と「可能性（他でありえたもの）」の差異として構成され、常に次の意味を指し示すものである。ここで重要なのは、ルーマンにおける「意味」が自己言及的であることである。ある意味が選択される際、「この意味を選択したこと自体」が次の意味の地平を開く。心的システムは、この再帰的な意味処理を継続的に行うことで自己を維持する。
\end{enumerate}

\subsection{AIは心的システムではない}

ルーマンの理論において、機械はオートポイエーシス・システム（生命・意識・社会）とは区別される「非ポイエティック」な存在とされている\cite{luhmann1984}。AIシステムも基本的には計算機プログラム（機械）上で動作するシステムである。AI——その最先端の形態を含めて——は心的システムに該当しない。

オートポイエティックでない。たとえ自己学習能力を持つAIであっても、その「学習」は人間が設計したアルゴリズムと訓練データに依存している。外部からの入力なしに自律的に自身の「思考」を産出し続ける閉鎖的なネットワークを持たない。Zenil (2026)が指摘するように、自己生成データへの依存度が高まると品質劣化が生じるという事実\cite{zenil2026}は、AIの「自己学習」が真に自己完結的なオートポイエーシスではないことを示唆している。

また、自己言及的な意味処理を行わない。AIの確率的出力は、文脈に応じて「もっともらしい」次のトークンを選択するが、この選択プロセスは自己言及的ではない。AIは「この意味を選択したこと自体」を次の処理の地平として開くことはなく、単に統計的パターンに基づいて出力を生成する\footnote{近年の実証研究もこの理論的予測と整合する。Mancoridis et al. (2025)は、LLMが概念を「理解しているフリ」をする現象を「ポチョムキン理解」として報告している\cite{vafa2025}。主要なLLMは概念説明タスクで高精度を示す一方、同じ概念を応用するタスクでは正答率が大きく低下した。また、Song et al. (2026)による包括的サーベイでは、LLMの推論失敗が認知バイアス・ワーキングメモリ限界・心の理論失敗など多岐にわたることが整理されており、「理解」の欠如が構造的な限界であることが示されている\cite{song2026}。}。

さらに、価値判断を伴わない。心的システムからの刺激は「何が重要か」「何を優先すべきか」という規範的判断を前提とするが、AIの出力は統計的パターンに基づいており、独自の規範的判断を伴わない。たとえAIが「倫理的判断」を行うように見えても、それは訓練データに含まれる人間の価値判断を統計的に再現しているに過ぎず、AI自体が価値を創出しているわけではない。

\subsection{構造的カップリングにおける質的差異}

井庭 (2024/2026) は、生成AIが「機能的等価物」として社会システムに対して機能しうると論じている\cite{iba2026}。確かに、AIが生成した応答はコミュニケーションの連鎖に組み込まれ、社会システムに対して「刺激」を与えることはありうる。

しかし、この点においても、心的システムとAIの間には質的な差異が存在する。

心的システムからの刺激は、「価値判断を伴う意味構成」として提供される。これは、「何が重要か」「何を優先すべきか」という規範的判断を前提としている。これに対し、AIからの刺激は統計的パターンマッチングに基づく「もっともらしい」応答であり、規範的判断を伴わない。

したがって、AIは社会システムに「刺激」を与えうるとしても、それが「理解の創造性」——価値判断を伴う意味構成——ではない\footnote{より理論的に言えば、構造的カップリングの「質」は、カップリングするシステムの認知構造に依存する。心的システムは、内部の選択構造を通じて社会的刺激を「フィルタリング」し、どの刺激を意味あるものとして取り込むかを選別する。この「選別」こそが価値判断の源泉である。これに対し、オートポイエティックでないシステムは、社会システムに「刺激」を与えうるとしても、そのカップリングは「機能的影響」や「インターフェース媒介的補助」にとどまり、「相互的自己産出的共進化」には参加できない\cite{esposito2001}。}。

なお、以上の議論は、AIからの刺激が人間の心的システムを触発し、人間が新たな価値を発見する可能性を否定するものではない。AIは「異質な他者性」としての刺激を提供し、人間の規範創造を加速させる「変異生成装置」として機能しうる。しかし、「理解」の主導権——価値判断を伴う意味の確定——はあくまで人間（心的システム）側にある。この点において、AIとの「協働」は可能であるが、「理解の創造性」への「参加」は不可能であるという区別が維持される。

\subsection{AIの限界：まとめ}

以上の議論から、公共政策におけるAIの原理的限界が明らかになった。AIは「情報」の選択や「伝達」の補助には機能しうるが、「理解」の創造的プロセスには原理的に参加できない。AIは心的システムを持たないため、価値判断を伴う意味構成を行えないのである。

この結論は、XAI研究からの実践的知見とも整合する。Keenan \& Sokol (2023)は、ルーマンの枠組みに基づき、AIの「説明」は主に「情報」と「伝達」の段階に留まっており、「理解」の段階は受け手の社会的・心理的文脈に依存することを指摘している\cite{keenan2023}。この指摘は、本稿がルーマン理論から演繹的に導いた「AIは理解の創造性に参加できない」という結論を、実証的研究の観点からも裏付けるものである。

%% ============================================================
%% 第5章 結論
%% ============================================================
\section{結論}

本稿は、AI時代において「人間による政策の意義」を理論的に基礎づけるため、「執政の創造性」という新たな概念を提起した。

「執政の創造性」とは、社会の構成員同士のコミュニケーションを前提に、価値判断を基盤として、新たなガバナンスの範囲を生成・配分していく能力である。この概念の重要性を、政策過程モデル（キングドン、サバティエ、村松）およびウィキッド・プロブレム論を通じて示した。

この創造性をルーマンの社会システム理論によって基礎づけた。「構造的カップリング」の概念を用いて、「理解の創造性」が心的システムから社会システムへの刺激として提供される価値判断を伴う意味構成のプロセスであることを明らかにした。

AIの原理的限界を論じた。AIは心的システムを持たないため、「情報」や「伝達」の補助には機能しうるが、「理解の創造性」には参加できない。この限界の根源は、AIが構造的カップリングにおいて質的に異なる存在であることにある。

この概念は、AIと人間の役割分担を明確にする。AIは「情報」や「伝達」の補助に機能しうるが、「理解」の創造的プロセスには参加できない。したがって、価値判断、合意形成、説明責任は人間が担うべきである。

今後の研究課題としては、(1)「執政の創造性」の実践的評価指標の確立、(2)AI活用と人間の価値判断を両立させる制度設計、(3)政策分野ごとの「執政の創造性」発揮形態の比較分析、などが挙げられる。

\section*{謝辞}
本稿は政策情報学会2024年研究発表会で報告した内容を発展させたものである。

%% ============================================================
%% 参考文献
%% ============================================================
\begin{thebibliography}{99}

\bibitem{soumu2022}
    総務省：自治体におけるAI活用・導入ガイドブック＜導入手順編＞、令和4年、2022年。
    \url{https://www.soumu.go.jp/main_content/000820109.pdf}

\bibitem{vafa2025}
    Marina Mancoridis, Bec Weeks, Keyon Vafa, Sendhil Mullainathan: Potemkin Understanding in Large Language Models, \textit{Proceedings of the 42nd International Conference on Machine Learning (ICML)}, PMLR 267, pp.42857--42881, 2025.

\bibitem{luhmann1984}
    Niklas Luhmann: \textit{Soziale Systeme}, Suhrkamp, 1984. [馬場靖雄訳：社会システム理論、勁草書房、2001--2020年]

\bibitem{luhmann1997}
    Niklas Luhmann: \textit{Die Gesellschaft der Gesellschaft}, Suhrkamp, 1997.

\bibitem{esposito2001}
    Elena Esposito: Strukturelle Kopplung mit unsichtbaren Maschinen, \textit{Soziale Systeme}, Vol.7, No.2, pp.241--252, 2001.

\bibitem{habermas1981}
    Jürgen Habermas: \textit{Theorie des kommunikativen Handelns}, Suhrkamp, 1981. [河上倫逸ほか訳：コミュニケーション的行為の理論、未来社、1985--1987年]

\bibitem{maturana1980}
    Humberto R. Maturana, Francisco J. Varela: \textit{Autopoiesis and Cognition: The Realization of the Living}, D. Reidel, 1980.

\bibitem{kneer1993}
    Georg Kneer, Armin Nassehi: \textit{Niklas Luhmanns Theorie sozialer Systeme}, Fink, 1993. [館野受男訳：ルーマン社会システム理論――「知」の扉をひらく、新泉社、1995年]

\bibitem{iba2009}
    Iba, T.: An Autopoietic Systems Theory for Creativity, \textit{The 1st Conference on Collaborative Innovation Networks (COINs)}, 2009.

\bibitem{iba2026}
    Takashi Iba: Why Is Creative Collaboration With Generative AI Actually Possible? A Theoretical Analysis With Social System Theory and Creative System Theory, \textit{Artificial Intelligence and Networks for a Sustainable Future}, pp.173--194, Springer, 2026.
    \url{https://doi.org/10.1007/978-3-032-13458-5_11}

\bibitem{canguilhem1966}
    Georges Canguilhem: \textit{Le normal et le pathologique}, PUF, 1966. [滝沢武久訳：正常と病理（叢書・ウニベルシタス, 225）、法政大学出版局、1987年]

\bibitem{kingdon1984}
    John W. Kingdon: \textit{Agendas, Alternatives, and Public Policies}, Little, Brown, 1984.

\bibitem{kusano1997}
    草野厚：政策過程分析入門、慶應義塾大学出版会、1997年。

\bibitem{sabatier1988}
    Paul A. Sabatier: An Advocacy Coalition Framework of Policy Change and the Role of Policy-Oriented Learning Therein, \textit{Policy Sciences}, Vol.21, No.2/3, pp.129--168, 1988.

\bibitem{muramatsu1981}
    村松岐夫：戦後日本の官僚制、東洋経済新報社、1981年。

\bibitem{rittel1973}
    Horst W. J. Rittel, Melvin M. Webber: Dilemmas in a General Theory of Planning, \textit{Policy Sciences}, Vol.4, No.2, pp.155--169, 1973.

\bibitem{sugitani2021}
    杉谷和哉：ウィキッド・プロブレムとしての新型コロナ感染症　政治と専門性の関係を中心に、『医療福祉政策研究』、第4巻、第1号、pp.27--37, 2021.

\bibitem{barocas2019}
    Solon Barocas, Moritz Hardt, Arvind Narayanan: \textit{Fairness and Machine Learning: Limitations and Opportunities}, fairmlbook.org, 2019.

\bibitem{parasuraman2008}
    Raja Parasuraman, Christopher D. Wickens: Humans: Still Vital After All These Years of Automation, \textit{Human Factors}, Vol.50, No.3, pp.511--520, 2008.

\bibitem{shneiderman2022}
    Ben Shneiderman: \textit{Human-Centered AI}, Oxford University Press, 2022.

\bibitem{song2026}
    Peiyang Song, Pengrui Han, Noah Goodman: Large Language Model Reasoning Failures, \textit{Transactions on Machine Learning Research}, 2026.
    \url{https://arxiv.org/abs/2602.06176}

\bibitem{simonds2025}
    Toby Simonds, Akira Yoshiyama: LADDER: Self-Improving LLMs Through Recursive Problem Decomposition, \textit{arXiv preprint arXiv:2503.00735}, 2025.
    \url{https://arxiv.org/abs/2503.00735}

\bibitem{qu2024}
    Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar: Recursive Introspection: Teaching Language Model Agents How to Self-Improve, \textit{arXiv preprint arXiv:2407.18219}, 2024.
    \url{https://arxiv.org/abs/2407.18219}

\bibitem{yin2024}
    Xunjian Yin, Xinyi Wang, Liangming Pan, Li Lin, Xiaojun Wan, William Yang Wang: Gödel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement, \textit{arXiv preprint arXiv:2410.04444}, 2024.
    \url{https://arxiv.org/abs/2410.04444}

\bibitem{zenil2026}
    Hector Zenil: On the Limits of Self-Improving in LLMs and Why AGI, ASI and the Singularity Are Not Near Without Symbolic Model Synthesis, \textit{arXiv preprint arXiv:2601.05280}, 2026.
    \url{https://arxiv.org/abs/2601.05280}

\bibitem{hedayati2024}
    Mona Hedayati: Human--Machine Relations Composed, Decomposed, Recomposed, \textit{Proceedings of the CHI Conference on Human Factors in Computing Systems}, 2024.

\bibitem{keenan2023}
    Bernard Keenan, Kacper B. Sokol: Mind the Gap! Bridging Explainable Artificial Intelligence and Human Understanding with Luhmann's Functional Theory of Communication, \textit{Journal of Artificial Intelligence Research}, Vol.78, pp.1--35, 2023.

\bibitem{kozuka2019}
    Souichirou Kozuka: A Governance Framework for the Development and Use of Artificial Intelligence: Lessons from the Comparison of Japanese and European Initiatives, \textit{European Journal of Law and Technology}, Vol.10, No.3, 2019.

\bibitem{ema2024}
    Akiyo Ema, Fujio Kudo, Yuka Iida, Toshihiro Jitsuzumi: Japan's Hiroshima AI Process: A Third Way in Global AI Governance, \textit{AI and Ethics}, 2024.

\bibitem{humphrey2024}
    David Humphrey: Japan's Society 5.0 and the Organizational Vision of ``Human-Centric'' AI, \textit{Science, Technology, \& Human Values}, 2024.

\end{thebibliography}

\end{document}
